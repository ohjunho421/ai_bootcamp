# 고객 데이터 분석 및 예측 모델 구축 과정

## 1. 데이터 준비 및 클러스터링

### 방법

- **StandardScaler**를 사용하여 데이터를 정규화
- **K-means 클러스터링** 알고리즘을 사용하여 고객을 6개의 클러스터로 분류

### 선택한 이유

- K-means는 구형 클러스터를 가정하며, 간단하고 효율적인 클러스터링 알고리즘입니다
- 클러스터 내 유사성을 최대화하고 클러스터 간 차이를 극대화하는 데 적합합니다

### 결과

- 클러스터 0~5로 나뉜 고객 그룹을 확인
- 각 클러스터의 평균 연간 소득(Annual Income)과 소비 점수(Spending Score)를 기준으로 군집화

---

## 2. 이상치 처리

### 질문

- 이상치를 어떤 기준으로 처리했나요?

### 답변

- **IQR (Interquartile Range)** 방법을 사용했습니다
    - 1사분위수(Q1)와 3사분위수(Q3)를 계산하여 IQR = Q3 - Q1을 구했습니다
    - 이상치 경계:
        - 하한값: Q1 - 1.5 * IQR
        - 상한값: Q3 + 1.5 * IQR
    - 이상치를 넘는 데이터를 제거했습니다

### 결과

- 이상치 제거 전 데이터 크기: **200개**
- 이상치 제거 후 데이터 크기: **198개**

---

## 3. 정규화

### 질문

- 어떤 정규화 방법을 사용했고, 얼마나 잘 되었는지 비교했나요?

### 답변

- 두 가지 방법으로 정규화를 진행하고 비교했습니다:
    - **StandardScaler** (표준화): 평균 0, 표준편차 1로 스케일링
    - **MinMaxScaler** (정규화): 데이터 값을 0~1 사이로 변환
- **박스플롯 시각화** 결과:
    - **StandardScaler**는 데이터의 분포를 유지하며 표준화
    - **MinMaxScaler**는 데이터 범위를 0~1 사이로 축소하여 정규화
    - 두 방법 모두 이상치 제거 후 데이터를 스케일링하는 데 효과적이었음

---

## 4. 클러스터링 모델 학습

### 질문

- 클러스터링 모델은 어떻게 학습되었고, 최적의 클러스터 개수를 어떻게 결정했나요?

### 답변

1. **모델 선택**:
    - K-means, Hierarchical Clustering, Gaussian Mixture Model(GMM), DBSCAN을 적용
2. **최적의 클러스터 개수 결정**:
    - 엘보우 방법과 Silhouette Score를 사용하여 k=6이 최적임을 확인
3. **평가 결과**:
    - K-means의 Silhouette Score가 상대적으로 높았으며, 클러스터 경계도 명확히 구분됨

---

## 5. 시계열 데이터 생성 및 분석

### 방법

- 12개월 동안 각 클러스터의 연간 소득과 소비 점수의 변화를 시뮬레이션
    - 소득: ±5k$의 랜덤 변화를 추가
    - 소비 점수: ±10의 랜덤 변화를 추가

### 선택한 이유

- 원래 데이터에는 시간적 정보가 포함되지 않았기 때문에, 시뮬레이션을 통해 가상 시계열 데이터를 생성
- 랜덤 변화를 통해 시간에 따른 고객 행동 변화를 모델링

### 결과

- 클러스터별로 12개월 동안의 평균 연간 소득과 소비 점수를 계산

---

## 6. 시계열 트렌드 분석

### 방법

- 각 클러스터의 월별 평균 연간 소득과 소비 점수를 계산
- 결과를 **라인 차트(Line Chart)**로 시각화

### 선택한 이유

- 트렌드 라인은 시간에 따른 평균 행동 변화를 명확히 보여줌
- 클러스터 간의 차이를 시각적으로 비교할 수 있음

### 결과

### 연간 소득 트렌드:

- 대부분의 클러스터에서 소득은 점진적으로 증가하거나 일정한 범위에서 유지
- 일부 클러스터(예: Cluster 3)는 변동성이 큼

### 소비 점수 트렌드:

- 특정 클러스터(예: Cluster 0, 5)는 소비 점수가 지속적으로 증가
- Cluster 2는 초기에는 높았으나 점차 감소하는 패턴

---

## 7. 결과 비교

| 클러스터 | 소득 트렌드 요약 | 소비 점수 트렌드 요약 |
| --- | --- | --- |
| Cluster 0 | 안정적 증가 | 점진적 증가 |
| Cluster 1 | 약간의 변동 | 하락 후 회복 |
| Cluster 2 | 변동성 있음 | 초기 높음, 점진적 감소 |
| Cluster 3 | 큰 변동 | 상승과 하락 반복 |
| Cluster 4 | 완만한 상승 | 지속적으로 증가 |
| Cluster 5 | 일정한 소득 유지 | 꾸준한 증가 |

---

## 8. 지도 학습 모델: 구매 가능성 예측

### 질문

- 구매 가능성을 예측하는 지도 학습 모델은 어떻게 구축되었나요?

### 답변

1. **타겟 변수 생성**:
    - 클러스터링 결과를 기반으로, 소비 점수가 높은 클러스터를 구매 가능성이 높은 고객으로 정의
    - `Purchase_Likelihood` 컬럼 추가 (0: 낮음, 1: 높음)
2. **데이터 분리**:
    - 학습 데이터(80%)와 테스트 데이터(20%)로 분리
3. **모델 학습**:
    - **Random Forest Classifier**를 사용해 학습
4. **결과 해석**:
    - Precision, Recall, F1-score 모두 **1.00**
    - 테스트 데이터 정확도(Accuracy) **1.00**
    - 데이터 불균형으로 인해 모델이 과적합(overfitting)될 가능성이 있음

---

## 9. 하이퍼파라미터 최적화

### 질문

- 하이퍼파라미터는 어떻게 설정해야 하나요?

### 답변

- **Random Forest 모델**의 주요 하이퍼파라미터:
    - `n_estimators`: 트리 개수, 더 많은 트리는 안정성을 증가
    - `max_depth`: 트리의 최대 깊이, 과적합 방지
    - `min_samples_split`: 노드를 분할하기 위한 최소 샘플 수
    - `min_samples_leaf`: 리프 노드에 필요한 최소 샘플 수
- **GridSearchCV**를 사용해 최적의 하이퍼파라미터를 탐색하는 것이 권장됩니다

---

## 10. 최종 결과 비교

| 단계 | 방법 | 결과 |
| --- | --- | --- |
| 이상치 처리 | IQR | 이상치 제거 후 데이터 크기: **198개** |
| 정규화 | StandardScaler, MinMaxScaler | 데이터 분포 유지(StandardScaler), 데이터 범위 축소(MinMaxScaler) |
| 클러스터링 모델 | K-means (k=6) | Silhouette Score: **0.43**, 클러스터 경계가 비교적 명확 |
| 지도 학습 모델 | Random Forest Classifier | 정확도: **1.00**, Precision/Recall/F1-score 모두 1.00 |
| 하이퍼파라미터 최적화 | GridSearchCV | `n_estimators`, `max_depth` 등 최적값 탐색 필요 |

---

## 11. 결론

1. **K-means**:
    - 데이터가 구형 분포에 가깝고, 클러스터 간 경계가 명확하여 적합한 클러스터링 기법으로 판단
2. **Random Forest Classifier**:
    - 구매 가능성 예측에 강력한 성능을 보였으나, 데이터 불균형 문제가 있으므로 추가 데이터 수집 또는 샘플링 기법 필요
3. **하이퍼파라미터 최적화**:
    - GridSearchCV를 통해 최적의 설정을 탐색하여 성능을 더욱 향상시킬 여지가 있음